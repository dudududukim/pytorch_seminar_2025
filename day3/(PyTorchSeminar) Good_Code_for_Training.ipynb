{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Learning Development Tools Tutorial\n",
        "\n",
        "**Author:** DuhyeonKim + Perplexity (Claude 4)\n",
        "\n",
        "This notebook demonstrates essential tools and techniques for machine learning development including progress tracking, device management, argument parsing, loss tracking, evaluation, and more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation Requirements\n",
        "\n",
        "First, let's install the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install torch torchvision tqdm tensorboard matplotlib pillow numpy scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. tqdm - Progress Tracking\n",
        "\n",
        "tqdm provides fast, extensible progress bars for loops and iterables.\n",
        "\n",
        "There are two ways to use tqdm.\n",
        "\n",
        "- First is using pbar and update by myself in the for loop.\n",
        "- Second is wrapping my iterable with tqdm so tqdm can track the process itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### using pdar.update(1)\n",
        "\n",
        "self update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|██████████| 100/100 [00:02<00:00, 41.25it/s]\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Simplified tqdm example using pbar.update()\n",
        "pbar = tqdm(total=100, desc='Processing')\n",
        "\n",
        "for i, _ in enumerate(range(100)):\n",
        "    time.sleep(0.02)  # Simulate delay\n",
        "    pbar.update(1)\n",
        "\n",
        "pbar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 10/10 [00:05<00:00,  1.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 completed. average loss: 0.0100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 10/10 [00:05<00:00,  1.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 completed. average loss: 0.0200\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 10/10 [00:05<00:00,  1.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 completed. average loss: 0.0300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Simple example applying tqdm with pbar.update() for epochs\n",
        "epochs = 3\n",
        "num_batches = 10\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    pbar = tqdm(total=num_batches, desc=f'Epoch {epoch}')\n",
        "    for batch_idx in range(num_batches):\n",
        "        time.sleep(0.5)  # Simulate delay -> originally training computation\n",
        "        pbar.update(1)\n",
        "    pbar.close()\n",
        "    \n",
        "    print(f\"Epoch {epoch} completed. average loss: {0.01 * epoch:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### wrapping iterable(list, tupe, dataframe, etc.) with tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing items: 100%|██████████| 1000/1000 [00:12<00:00, 82.41it/s]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "long_array = [random.randint(1, 1000) for _ in range(1000)]\n",
        "\n",
        "for item in tqdm(long_array, desc='Processing items'):\n",
        "    time.sleep(0.01)\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Device Management\n",
        "\n",
        "Proper device management for GPU/CPU operations.\n",
        "\n",
        "> ⚠️ **Warning**: one of the tensors is not allocated to device -> error occurs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: mps\n",
            "Input device: mps:0\n",
            "Model device: mps:0\n",
            "Output device: mps:0\n",
            "Output shape: torch.Size([5, 1])\n",
            "All available mps devices:\n",
            "  - mps:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Simple device detection using accelerator\n",
        "# device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Create a simple model and move to device\n",
        "model = nn.Linear(10, 1).to(device)\n",
        "\n",
        "# Create sample data and move to device\n",
        "x = torch.randn(5, 10).to(device)\n",
        "# x = torch.randn(5, 10)\n",
        "\n",
        "# Forward pass\n",
        "output = model(x)\n",
        "\n",
        "# Print meaningful information\n",
        "print(f\"Input device: {x.device}\")\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "print(f\"Output device: {output.device}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "\n",
        "# Print all available devices if cuda or mps\n",
        "if device == 'cuda':\n",
        "    device_count = torch.cuda.device_count()\n",
        "    print(\"All available cuda devices:\")\n",
        "    for i in range(device_count):\n",
        "        print(f\"  - cuda:{i}\")\n",
        "elif device == 'mps':\n",
        "    print(\"All available mps devices:\")\n",
        "    print(\"  - mps:0\")  # f-string 제거\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. argparse - Command Line Arguments\n",
        "\n",
        "argparse is used for parsing command-line arguments in terminal scripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "usage: ipykernel_launcher.py [-h] --model MODEL --mode {train,test}\n",
            "                             --dataset DATASET [--epoch EPOCH]\n",
            "ipykernel_launcher.py: error: the following arguments are required: --model, --mode, --dataset\n"
          ]
        },
        {
          "ename": "SystemExit",
          "evalue": "2",
          "output_type": "error",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kimduhyeon/Desktop/d2f/d2f/lib/python3.13/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser(description='Simple training script')\n",
        "    parser.add_argument('--model', type=str, required=True, help='Model name')\n",
        "    parser.add_argument('--mode', type=str, choices=['train', 'test'], required=True, help='Mode: train or test')\n",
        "    parser.add_argument('--dataset', type=str, required=True, help='Dataset name')\n",
        "    parser.add_argument('--epoch', type=int, default=10, help='Number of epochs')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    print(f\"Model: {args.model}\")\n",
        "    print(f\"Mode: {args.mode}\")\n",
        "    print(f\"Dataset: {args.dataset}\")\n",
        "    print(f\"Epochs: {args.epoch}\")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Loss Tracking\n",
        "\n",
        "> 훈련 손실 (Training Loss)\n",
        "\n",
        "- `train_loss_step`: 개별 배치의 즉시 손실값\n",
        "- `train_loss_epoch`: 전체 에포크에 걸친 가중 평균 손실  \n",
        "- `avg_train_loss`: 모든 스텝 손실의 평균값\n",
        "\n",
        "> 검증 손실 (Validation Loss)\n",
        "\n",
        "- `val_loss`: 개별 검증 배치의 손실\n",
        "- `val_loss_epoch`: 검증 에포크 전체의 손실\n",
        "- `avg_val_loss`: 검증 손실의 평균값\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Evaluation Stage\n",
        "\n",
        "**Why Evaluation is Necessary**\n",
        "\n",
        "1. **Overfitting Detection**\n",
        "   Through evaluation metrics, we can detect overfitting where models perform well only on training data but poorly on new data.\n",
        "\n",
        "2. **Model Comparison and Selection**\n",
        "   We can select the most suitable model for a given problem among various algorithms or hyperparameter configurations.\n",
        "\n",
        "3. **Accuracy Assurance**\n",
        "   Ensures that the model can make reliable predictions in real-world environments. High accuracy indicates excellent predictive power of the model.\n",
        "\n",
        "4. **Deployment Decision Support**\n",
        "   Evaluation results provide crucial information for deciding whether to deploy the model to production environments.\n",
        "\n",
        "**Evaluation Methodologies**\n",
        "\n",
        "**Hold-out Validation**\n",
        "The simplest method that separates the dataset into training and testing sets for evaluation. The model is built on the training set and evaluated on the test set.\n",
        "\n",
        "**Cross-validation**\n",
        "A more robust evaluation method that reduces performance variations due to data splitting approaches.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Computational Efficiency\n",
        "\n",
        "Tools for measuring and optimizing computational efficiency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from PIL import Image\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"        # device=\"mps\"\n",
        "print(\"using device:\", device)\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        \n",
        "        self.samples = []\n",
        "        self.class_to_idx = {}\n",
        "\n",
        "        # Filter out hidden files/folders starting with ._ or .\n",
        "        classes = sorted([cls for cls in os.listdir(root) \n",
        "                         if not cls.startswith('.') and os.path.isdir(os.path.join(root, cls))])\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
        "\n",
        "        for cls_name in classes:\n",
        "            cls_folder = os.path.join(root, cls_name)\n",
        "            for fname in os.listdir(cls_folder):\n",
        "                # Skip files starting with ._\n",
        "                if fname.startswith('.'):\n",
        "                    continue\n",
        "                    \n",
        "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    path = os.path.join(cls_folder, fname)\n",
        "                    self.samples.append((path, self.class_to_idx[cls_name]))\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print(self.class_to_idx)\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, num_classes=10):  # MODIFIED: Made number of classes as parameter\n",
        "        super(CustomModel, self).__init__()\n",
        "        \n",
        "        # MODIFIED: Improved with deeper network architecture\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)  # MODIFIED: Added Batch Normalization\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)  # MODIFIED: Added Batch Normalization\n",
        "        \n",
        "        # MODIFIED: Additional convolutional layer\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        \n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        # MODIFIED: Adjusted dropout rate and applied to multiple locations\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        \n",
        "        # MODIFIED: Increased fully connected layer size\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)  # 32x32 -> 16x16 -> 8x8 -> 4x4\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # MODIFIED: Improved forward pass with Batch Normalization\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # 32x32 -> 16x16\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # 16x16 -> 8x8\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # 8x8 -> 4x4\n",
        "        \n",
        "        x = self.dropout1(x)\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class CustomModel_simple(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(CustomModel_simple, self).__init__()\n",
        "        self.fc = nn.Linear(3 * 32 * 32, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# ----------------- dataset and dataloader -----------------\n",
        "train_root = '../day2/cifar10_images/train'\n",
        "test_root = '../day2/cifar10_images/test'\n",
        "\n",
        "# external disk\n",
        "# train_root = '/Volumes/T7 Shield/cifar10_images/train'\n",
        "# test_root = '/Volumes/T7 Shield/cifar10_images/test'\n",
        "\n",
        "# MODIFIED: Enhanced data augmentation techniques and added normalization\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomCrop(32, padding=4),  # MODIFIED: Added random crop\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # MODIFIED: ImageNet standard normalization\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # MODIFIED: Added normalization\n",
        "])\n",
        "\n",
        "train_dataset = CustomDataset(train_root, transform=train_transform)\n",
        "test_dataset = CustomDataset(test_root, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=True)\n",
        "\n",
        "# ----------------- model and training setup -----------------\n",
        "# MODIFIED: Calculate number of classes\n",
        "num_classes = len(train_dataset.class_to_idx)\n",
        "model = CustomModel(num_classes=num_classes).to(device)\n",
        "# model = CustomModel_simple(num_classes=num_classes).to(device)\n",
        "\n",
        "# ----------------- training loop -----------------\n",
        "total_epochs = 2\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# ‼️ optimizer modi SGD to Adam for better convergence\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "train_losses = []  # MODIFIED: List to store training losses\n",
        "compute_efficiencies = []\n",
        "\n",
        "try:\n",
        "    for epoch in range(total_epochs):\n",
        "        model.train()\n",
        "        # MODIFIED: added running loss for each epoch\n",
        "        running_loss = 0.0\n",
        "        pbar = tqdm(total=len(train_loader))\n",
        "        start_time = time.time()\n",
        "\n",
        "        for i, (images, labels) in enumerate(train_loader):             # enumerate returns both index(i) and value( (images, labels) )\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            prepare_time = time.time() - start_time\n",
        "            compute_start = time.time()\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            process_time = time.time() - compute_start\n",
        "            total_time = prepare_time + process_time\n",
        "            compute_efficiency = process_time / total_time\n",
        "            compute_efficiencies.append(compute_efficiency)\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            pbar.set_description(\n",
        "                f\"Epoch {epoch+1}/{total_epochs} | \"\n",
        "                f\"Compute Eff: {compute_efficiency:.3f} | \"\n",
        "                f\"Loss: {loss.item():.4f} | \"\n",
        "                f\"Prepare Time: {prepare_time:.3f}s | \"\n",
        "                f\"Process Time: {process_time:.3f}s | \"\n",
        "            )\n",
        "            pbar.update(1)  # Manual step update\n",
        "            start_time = time.time()\n",
        "\n",
        "\n",
        "        pbar.close()  # Close progress bar after epoch\n",
        "\n",
        "        avg_compute_efficiency = sum(compute_efficiencies[-len(train_loader):]) / len(train_loader)\n",
        "        avg_loss = running_loss / len(train_loader)\n",
        "        train_losses.append(avg_loss)\n",
        "\n",
        "        print(f\"Epoch [{epoch + 1}/{total_epochs}]  Train Loss: {avg_loss:.4f} | \"\n",
        "            f\"Avg Compute Efficiency: {avg_compute_efficiency:.3f}\")\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Training interrupted by user.\")\n",
        "    pbar.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. TensorBoard Integration\n",
        "\n",
        "Logging and visualizing training progress with TensorBoard.\n",
        "\n",
        "> Let's use util.py (external file) and import from it.\n",
        "\n",
        "It can be used to track gradient vanishing problems."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using device: mps\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/50 | Loss: 1.1249: 100%|██████████| 391/391 [00:23<00:00, 16.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50]  Train Loss: 1.5181\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/50 | Loss: 1.1498: 100%|██████████| 391/391 [00:22<00:00, 17.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [2/50]  Train Loss: 1.1771\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/50 | Loss: 0.8370: 100%|██████████| 391/391 [00:23<00:00, 16.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [3/50]  Train Loss: 1.0427\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/50 | Loss: 0.7313: 100%|██████████| 391/391 [00:22<00:00, 17.73it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [4/50]  Train Loss: 0.9593\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/50 | Loss: 1.0640: 100%|██████████| 391/391 [00:22<00:00, 17.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [5/50]  Train Loss: 0.9107\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/50 | Loss: 1.1872: 100%|██████████| 391/391 [00:23<00:00, 16.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [6/50]  Train Loss: 0.8704\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 7/50 | Loss: 1.0390: 100%|██████████| 391/391 [00:24<00:00, 16.02it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [7/50]  Train Loss: 0.8302\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 8/50 | Loss: 0.9002: 100%|██████████| 391/391 [00:25<00:00, 15.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [8/50]  Train Loss: 0.7984\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 9/50 | Loss: 0.7237: 100%|██████████| 391/391 [00:20<00:00, 19.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [9/50]  Train Loss: 0.7806\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 10/50 | Loss: 0.6132: 100%|██████████| 391/391 [00:17<00:00, 22.95it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [10/50]  Train Loss: 0.7492\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 11/50 | Loss: 0.8217: 100%|██████████| 391/391 [00:17<00:00, 21.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [11/50]  Train Loss: 0.7341\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 12/50 | Loss: 0.6123: 100%|██████████| 391/391 [00:18<00:00, 20.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [12/50]  Train Loss: 0.7113\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 13/50 | Loss: 0.6913: 100%|██████████| 391/391 [00:19<00:00, 20.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [13/50]  Train Loss: 0.7012\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 14/50 | Loss: 0.6913: 100%|██████████| 391/391 [00:19<00:00, 19.56it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [14/50]  Train Loss: 0.6800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 15/50 | Loss: 0.5594: 100%|██████████| 391/391 [00:21<00:00, 18.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [15/50]  Train Loss: 0.6718\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 16/50 | Loss: 0.6538: 100%|██████████| 391/391 [00:21<00:00, 18.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [16/50]  Train Loss: 0.6574\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 17/50 | Loss: 0.6174: 100%|██████████| 391/391 [00:21<00:00, 17.98it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [17/50]  Train Loss: 0.6420\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 18/50 | Loss: 0.5780: 100%|██████████| 391/391 [00:22<00:00, 17.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [18/50]  Train Loss: 0.6305\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 19/50 | Loss: 0.6982: 100%|██████████| 391/391 [00:22<00:00, 17.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [19/50]  Train Loss: 0.6207\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 20/50 | Loss: 0.6058: 100%|██████████| 391/391 [00:22<00:00, 17.31it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [20/50]  Train Loss: 0.6143\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 21/50 | Loss: 0.8273: 100%|██████████| 391/391 [00:20<00:00, 19.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [21/50]  Train Loss: 0.5987\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 22/50 | Loss: 0.6465: 100%|██████████| 391/391 [00:20<00:00, 18.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [22/50]  Train Loss: 0.5917\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 23/50 | Loss: 0.5781: 100%|██████████| 391/391 [00:26<00:00, 14.99it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [23/50]  Train Loss: 0.5840\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 24/50 | Loss: 0.7967: 100%|██████████| 391/391 [00:22<00:00, 17.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [24/50]  Train Loss: 0.5781\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 25/50 | Loss: 0.4022: 100%|██████████| 391/391 [00:23<00:00, 17.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [25/50]  Train Loss: 0.5706\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 26/50 | Loss: 1.0024: 100%|██████████| 391/391 [00:22<00:00, 17.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [26/50]  Train Loss: 0.5643\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 27/50 | Loss: 0.6077: 100%|██████████| 391/391 [00:23<00:00, 16.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [27/50]  Train Loss: 0.5586\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 28/50 | Loss: 0.5017: 100%|██████████| 391/391 [00:23<00:00, 16.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [28/50]  Train Loss: 0.5486\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 29/50 | Loss: 0.5423: 100%|██████████| 391/391 [00:25<00:00, 15.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [29/50]  Train Loss: 0.5488\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 30/50 | Loss: 0.7128: 100%|██████████| 391/391 [00:27<00:00, 14.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [30/50]  Train Loss: 0.5415\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 31/50 | Loss: 0.4670:  48%|████▊     | 187/391 [00:13<00:14, 13.92it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[16], line 143\u001b[0m\n\u001b[1;32m    141\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m    142\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 143\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# logger logging\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/d2f/d2f/lib/python3.13/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
            "File \u001b[0;32m~/Desktop/d2f/d2f/lib/python3.13/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
            "File \u001b[0;32m~/Desktop/d2f/d2f/lib/python3.13/site-packages/torch/optim/adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    235\u001b[0m         group,\n\u001b[1;32m    236\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m         state_steps,\n\u001b[1;32m    242\u001b[0m     )\n\u001b[0;32m--> 244\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
            "File \u001b[0;32m~/Desktop/d2f/d2f/lib/python3.13/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/d2f/d2f/lib/python3.13/site-packages/torch/optim/adam.py:876\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    874\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 876\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/Desktop/d2f/d2f/lib/python3.13/site-packages/torch/optim/adam.py:425\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    423\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m device_beta1)\n\u001b[0;32m--> 425\u001b[0m \u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    428\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "from util import Logger\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root, transform=None):\n",
        "        self.root = root\n",
        "        self.transform = transform\n",
        "        \n",
        "        self.samples = []\n",
        "        self.class_to_idx = {}\n",
        "\n",
        "        classes = sorted(os.listdir(root))\n",
        "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
        "\n",
        "        for cls_name in classes:\n",
        "            cls_folder = os.path.join(root, cls_name)\n",
        "            for fname in os.listdir(cls_folder):\n",
        "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                    path = os.path.join(cls_folder, fname)\n",
        "                    self.samples.append((path, self.class_to_idx[cls_name]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # print(self.class_to_idx)\n",
        "        path, label = self.samples[idx]\n",
        "        img = Image.open(path)\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, label\n",
        "\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, num_classes=10):  # MODIFIED: Made number of classes as parameter\n",
        "        super(CustomModel, self).__init__()\n",
        "        \n",
        "        # MODIFIED: Improved with deeper network architecture\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(32)  # MODIFIED: Added Batch Normalization\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)  # MODIFIED: Added Batch Normalization\n",
        "        \n",
        "        # MODIFIED: Additional convolutional layer\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        \n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        # MODIFIED: Adjusted dropout rate and applied to multiple locations\n",
        "        self.dropout1 = nn.Dropout(0.3)\n",
        "        self.dropout2 = nn.Dropout(0.3)\n",
        "        \n",
        "        # MODIFIED: Increased fully connected layer size\n",
        "        self.fc1 = nn.Linear(128 * 4 * 4, 512)  # 32x32 -> 16x16 -> 8x8 -> 4x4\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # MODIFIED: Improved forward pass with Batch Normalization\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # 32x32 -> 16x16\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # 16x16 -> 8x8\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # 8x8 -> 4x4\n",
        "        \n",
        "        x = self.dropout1(x)\n",
        "        x = x.view(-1, 128 * 4 * 4)\n",
        "        \n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# ----------------- dataset and dataloader -----------------\n",
        "train_root = '../day2/cifar10_images/train'\n",
        "test_root = '../day2/cifar10_images/test'\n",
        "\n",
        "# MODIFIED: Enhanced data augmentation techniques and added normalization\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomCrop(32, padding=4),  # MODIFIED: Added random crop\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # MODIFIED: ImageNet standard normalization\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # MODIFIED: Added normalization\n",
        "])\n",
        "\n",
        "train_dataset = CustomDataset(train_root, transform=train_transform)\n",
        "test_dataset = CustomDataset(test_root, transform=test_transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=True)\n",
        "\n",
        "# ----------------- model and training setup -----------------\n",
        "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"        # device=\"mps\"\n",
        "\n",
        "# MODIFIED: Calculate number of classes\n",
        "num_classes = len(train_dataset.class_to_idx)\n",
        "model = CustomModel(num_classes=num_classes).to(device)\n",
        "\n",
        "# ----------------- training loop -----------------\n",
        "total_epochs = 50\n",
        "print(\"using device:\", device)\n",
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "\n",
        "# ‼️ optimizer modi SGD to Adam for better convergence\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "train_losses = []  # MODIFIED: List to store training losses\n",
        "\n",
        "# logger\n",
        "logger = Logger(log_dir='./logs')\n",
        "global_step = 0\n",
        "\n",
        "for epoch in range(total_epochs):\n",
        "    model.train()\n",
        "    # MODIFIED: added running loss for each epoch\n",
        "    running_loss = 0.0\n",
        "    \n",
        "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
        "\n",
        "    for i, (images, labels) in pbar:             # enumerate returns both index(i) and value( (images, labels) )\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # logger logging\n",
        "        logger.scalar_summary('Train/Loss_Step', loss.item(), global_step)\n",
        "\n",
        "        if epoch == 0 and i == 0:\n",
        "            mean = torch.tensor([0.4914, 0.4822, 0.4465]).view(3, 1, 1).to(device)\n",
        "            std = torch.tensor([0.2023, 0.1994, 0.2010]).view(3, 1, 1).to(device)\n",
        "            sample_images = images[:8].clone()\n",
        "            sample_images = sample_images * std + mean\n",
        "            sample_images = torch.clamp(sample_images, 0, 1)\n",
        "        \n",
        "        logger.images_summary('Train/Sample_Images', sample_images, global_step)\n",
        "\n",
        "        pbar.set_description(f\"Epoch {epoch+1}/{total_epochs} | Loss: {loss.item():.4f}\")\n",
        "\n",
        "        global_step += 1\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    train_losses.append(avg_loss)\n",
        "\n",
        "    # logger logging\n",
        "    logger.scalar_summary('Train/Loss_Epoch', avg_loss, epoch)\n",
        "    logger.scalar_summary('Train/Learning_Rate', optimizer.param_groups[0]['lr'], epoch)\n",
        "\n",
        "    # loggger logging parameters and gradients\n",
        "    if epoch % 5 == 0:\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                logger.histo_summary(f'Parameters/{name}', param.data, epoch)\n",
        "                logger.histo_summary(f'Gradients/{name}', param.grad.data, epoch)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{total_epochs}]  Train Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Additional Features\n",
        "\n",
        "Network initialization, schedulers, checkpointing, and GIF creation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Network Initialization\n",
        "\n",
        "$$\n",
        "a = \\text{gain} \\times \\sqrt{\\frac{6}{\\text{fan\\_in} + \\text{fan\\_out}}}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "w = torch.empty(3, 5)\n",
        "\n",
        "print(\"Xavier(Glorot) Normal Initialization:\")\n",
        "nn.init.xavier_normal_(w, gain=0.2)\n",
        "print(w)\n",
        "\n",
        "print(\"Xavier(Glorot) Uniform Initialization:\")\n",
        "nn.init.xavier_uniform_(w, gain=1.0)\n",
        "print(w)\n",
        "\n",
        "print(\"\\nKaiming(He) Normal Initialization (fan_in, relu):\")\n",
        "nn.init.kaiming_normal_(w, a=0, mode='fan_in', nonlinearity='relu')\n",
        "print(w)\n",
        "\n",
        "print(\"\\nKaiming(He) Uniform Initialization (fan_in, relu):\")\n",
        "nn.init.kaiming_uniform_(w, a=0, mode='fan_in', nonlinearity='relu')\n",
        "print(w)\n",
        "\n",
        "print(\"\\nOrthogonal Initialization:\")\n",
        "nn.init.orthogonal_(w, gain=0.2)\n",
        "print(w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "import math\n",
        "\n",
        "class NetworkInitializer:\n",
        "    \"\"\"Utility class for different network initialization strategies\"\"\"\n",
        "    \n",
        "    @staticmethod\n",
        "    def xavier_normal(module):\n",
        "        \"\"\"Xavier/Glorot normal initialization\"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "            init.xavier_normal_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                init.constant_(module.bias, 0)\n",
        "    \n",
        "    @staticmethod\n",
        "    def xavier_uniform(module):\n",
        "        \"\"\"Xavier/Glorot uniform initialization\"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "            init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                init.constant_(module.bias, 0)\n",
        "    \n",
        "    @staticmethod\n",
        "    def kaiming_normal(module):\n",
        "        \"\"\"Kaiming/He normal initialization\"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "            init.kaiming_normal_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "            if module.bias is not None:\n",
        "                init.constant_(module.bias, 0)\n",
        "    \n",
        "    @staticmethod\n",
        "    def kaiming_uniform(module):\n",
        "        \"\"\"Kaiming/He uniform initialization\"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "            init.kaiming_uniform_(module.weight, mode='fan_out', nonlinearity='relu')\n",
        "            if module.bias is not None:\n",
        "                init.constant_(module.bias, 0)\n",
        "    \n",
        "    @staticmethod\n",
        "    def normal_init(module, mean=0.0, std=0.01):\n",
        "        \"\"\"Normal initialization with specified mean and std\"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "            init.normal_(module.weight, mean, std)\n",
        "            if module.bias is not None:\n",
        "                init.constant_(module.bias, 0)\n",
        "    \n",
        "    @staticmethod\n",
        "    def orthogonal_init(module):\n",
        "        \"\"\"Orthogonal initialization\"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
        "            init.orthogonal_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                init.constant_(module.bias, 0)\n",
        "\n",
        "def initialize_model(model, init_type='kaiming_normal'):\n",
        "    \"\"\"Initialize model with specified initialization type\"\"\"\n",
        "    init_functions = {\n",
        "        'xavier_normal': NetworkInitializer.xavier_normal,\n",
        "        'xavier_uniform': NetworkInitializer.xavier_uniform,\n",
        "        'kaiming_normal': NetworkInitializer.kaiming_normal,\n",
        "        'kaiming_uniform': NetworkInitializer.kaiming_uniform,\n",
        "        'normal': NetworkInitializer.normal_init,\n",
        "        'orthogonal': NetworkInitializer.orthogonal_init\n",
        "    }\n",
        "    \n",
        "    if init_type not in init_functions:\n",
        "        raise ValueError(f\"Unknown initialization type: {init_type}\")\n",
        "    \n",
        "    model.apply(init_functions[init_type])\n",
        "    print(f\"Model initialized with {init_type}\")\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "class ExampleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Test different initializations\n",
        "for init_type in ['xavier_normal', 'kaiming_normal', 'orthogonal']:\n",
        "    model = ExampleNet()\n",
        "    model = initialize_model(model, init_type)\n",
        "    \n",
        "    # Check weight statistics\n",
        "    conv1_weights = model.conv1.weight.data\n",
        "    fc1_weights = model.fc1.weight.data\n",
        "    \n",
        "    print(f\"  Conv1 weights - Mean: {conv1_weights.mean():.6f}, Std: {conv1_weights.std():.6f}\")\n",
        "    print(f\"  FC1 weights - Mean: {fc1_weights.mean():.6f}, Std: {fc1_weights.std():.6f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Learning Rate Schedulers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def demonstrate_schedulers():\n",
        "    \"\"\"Demonstrate different learning rate schedulers\"\"\"\n",
        "    \n",
        "    # Create a dummy model and optimizer\n",
        "    model = nn.Linear(10, 1)\n",
        "    initial_lr = 0.1\n",
        "    epochs = 50\n",
        "    \n",
        "    schedulers_config = {\n",
        "        'StepLR': {\n",
        "            'optimizer': optim.SGD(model.parameters(), lr=initial_lr),\n",
        "            'scheduler_class': lr_scheduler.StepLR,\n",
        "            'scheduler_kwargs': {'step_size': 10, 'gamma': 0.5}\n",
        "        },\n",
        "        'ExponentialLR': {\n",
        "            'optimizer': optim.SGD(model.parameters(), lr=initial_lr),\n",
        "            'scheduler_class': lr_scheduler.ExponentialLR,\n",
        "            'scheduler_kwargs': {'gamma': 0.95}\n",
        "        },\n",
        "        'CosineAnnealingLR': {\n",
        "            'optimizer': optim.SGD(model.parameters(), lr=initial_lr),\n",
        "            'scheduler_class': lr_scheduler.CosineAnnealingLR,\n",
        "            'scheduler_kwargs': {'T_max': epochs}\n",
        "        },\n",
        "        'ReduceLROnPlateau': {\n",
        "            'optimizer': optim.SGD(model.parameters(), lr=initial_lr),\n",
        "            'scheduler_class': lr_scheduler.ReduceLROnPlateau,\n",
        "            'scheduler_kwargs': {'mode': 'min', 'factor': 0.5, 'patience': 5}\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for name, config in schedulers_config.items():\n",
        "        # Recreate optimizer for each scheduler\n",
        "        optimizer = optim.SGD(model.parameters(), lr=initial_lr)\n",
        "        scheduler = config['scheduler_class'](optimizer, **config['scheduler_kwargs'])\n",
        "        \n",
        "        learning_rates = []\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            # Record current learning rate\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            learning_rates.append(current_lr)\n",
        "            \n",
        "            # Simulate training step\n",
        "            optimizer.zero_grad()\n",
        "            # Dummy loss\n",
        "            loss = torch.tensor(1.0 - epoch * 0.01 + 0.1 * np.sin(epoch * 0.3))\n",
        "            \n",
        "            # Step scheduler\n",
        "            if name == 'ReduceLROnPlateau':\n",
        "                scheduler.step(loss)\n",
        "            else:\n",
        "                scheduler.step()\n",
        "        \n",
        "        results[name] = learning_rates\n",
        "    \n",
        "    # Plot results\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    for i, (name, lrs) in enumerate(results.items()):\n",
        "        axes[i].plot(range(epochs), lrs, linewidth=2, marker='o', markersize=3)\n",
        "        axes[i].set_title(f'{name}')\n",
        "        axes[i].set_xlabel('Epoch')\n",
        "        axes[i].set_ylabel('Learning Rate')\n",
        "        axes[i].grid(True, alpha=0.3)\n",
        "        axes[i].set_yscale('log')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Custom scheduler example\n",
        "class WarmupCosineScheduler:\n",
        "    \"\"\"Custom scheduler with warmup and cosine annealing\"\"\"\n",
        "    \n",
        "    def __init__(self, optimizer, warmup_epochs, total_epochs, min_lr=1e-6):\n",
        "        self.optimizer = optimizer\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.total_epochs = total_epochs\n",
        "        self.min_lr = min_lr\n",
        "        self.base_lr = optimizer.param_groups[0]['lr']\n",
        "        self.current_epoch = 0\n",
        "    \n",
        "    def step(self):\n",
        "        if self.current_epoch < self.warmup_epochs:\n",
        "            # Warmup phase\n",
        "            lr = self.base_lr * (self.current_epoch + 1) / self.warmup_epochs\n",
        "        else:\n",
        "            # Cosine annealing phase\n",
        "            progress = (self.current_epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)\n",
        "            lr = self.min_lr + (self.base_lr - self.min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "        \n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "        \n",
        "        self.current_epoch += 1\n",
        "    \n",
        "    def get_lr(self):\n",
        "        return [param_group['lr'] for param_group in self.optimizer.param_groups]\n",
        "\n",
        "# Test custom scheduler\n",
        "model = nn.Linear(10, 1)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "custom_scheduler = WarmupCosineScheduler(optimizer, warmup_epochs=10, total_epochs=50)\n",
        "\n",
        "custom_lrs = []\n",
        "for epoch in range(50):\n",
        "    custom_lrs.append(optimizer.param_groups[0]['lr'])\n",
        "    custom_scheduler.step()\n",
        "\n",
        "# Plot custom scheduler\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(50), custom_lrs, linewidth=2, marker='o', markersize=3)\n",
        "plt.title('Custom Warmup + Cosine Annealing Scheduler')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Learning Rate')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.axvline(x=10, color='red', linestyle='--', alpha=0.7, label='End of Warmup')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Demonstrate all schedulers\n",
        "print(\"Learning Rate Scheduler Comparison:\")\n",
        "scheduler_results = demonstrate_schedulers()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Checkpointing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "checkpoint = {\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'epoch': epoch,\n",
        "    # 기타 정보\n",
        "}\n",
        "torch.save(checkpoint, 'checkpoint.pth')\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "import glob\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "\n",
        "class ModelCheckpoint:\n",
        "    \"\"\"Comprehensive model checkpointing utility\"\"\"\n",
        "    \n",
        "    def __init__(self, save_dir='checkpoints', save_best=True, save_last=True, \n",
        "                 monitor='val_loss', mode='min', save_top_k=3):\n",
        "        self.save_dir = save_dir\n",
        "        self.save_best = save_best\n",
        "        self.save_last = save_last\n",
        "        self.monitor = monitor\n",
        "        self.mode = mode\n",
        "        self.save_top_k = save_top_k\n",
        "        \n",
        "        # Create save directory\n",
        "        os.makedirs(save_dir, exist_ok=True)\n",
        "        \n",
        "        # Track best metrics\n",
        "        self.best_metric = float('inf') if mode == 'min' else float('-inf')\n",
        "        self.saved_checkpoints = []\n",
        "    \n",
        "    def save_checkpoint(self, model, optimizer, scheduler, epoch, metrics, \n",
        "                       additional_state=None, filename=None):\n",
        "        \"\"\"Save model checkpoint\"\"\"\n",
        "        \n",
        "        if filename is None:\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "            filename = f\"checkpoint_epoch_{epoch:03d}_{timestamp}.pth\"\n",
        "        \n",
        "        filepath = os.path.join(self.save_dir, filename)\n",
        "        \n",
        "        # Prepare checkpoint data\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "            'metrics': metrics,\n",
        "            'timestamp': datetime.now().isoformat()\n",
        "        }\n",
        "        \n",
        "        if additional_state:\n",
        "            checkpoint['additional_state'] = additional_state\n",
        "        \n",
        "        # Save checkpoint\n",
        "        torch.save(checkpoint, filepath)\n",
        "        \n",
        "        # Check if this is the best model\n",
        "        current_metric = metrics.get(self.monitor)\n",
        "        is_best = False\n",
        "        \n",
        "        if current_metric is not None:\n",
        "            if ((self.mode == 'min' and current_metric < self.best_metric) or\n",
        "                (self.mode == 'max' and current_metric > self.best_metric)):\n",
        "                self.best_metric = current_metric\n",
        "                is_best = True\n",
        "                \n",
        "                if self.save_best:\n",
        "                    best_filepath = os.path.join(self.save_dir, 'best_model.pth')\n",
        "                    torch.save(checkpoint, best_filepath)\n",
        "                    print(f\"New best model saved: {self.monitor} = {current_metric:.6f}\")\n",
        "        \n",
        "        # Save last model\n",
        "        if self.save_last:\n",
        "            last_filepath = os.path.join(self.save_dir, 'last_model.pth')\n",
        "            torch.save(checkpoint, last_filepath)\n",
        "        \n",
        "        # Manage top-k checkpoints\n",
        "        if current_metric is not None:\n",
        "            self.saved_checkpoints.append({\n",
        "                'filepath': filepath,\n",
        "                'metric': current_metric,\n",
        "                'epoch': epoch\n",
        "            })\n",
        "            \n",
        "            # Sort by metric and keep only top-k\n",
        "            reverse = (self.mode == 'max')\n",
        "            self.saved_checkpoints.sort(key=lambda x: x['metric'], reverse=reverse)\n",
        "            \n",
        "            if len(self.saved_checkpoints) > self.save_top_k:\n",
        "                # Remove worst checkpoint file\n",
        "                worst_checkpoint = self.saved_checkpoints.pop()\n",
        "                if os.path.exists(worst_checkpoint['filepath']):\n",
        "                    os.remove(worst_checkpoint['filepath'])\n",
        "                    print(f\"Removed checkpoint: {worst_checkpoint['filepath']}\")\n",
        "        \n",
        "        print(f\"Checkpoint saved: {filepath}\")\n",
        "        return filepath, is_best\n",
        "    \n",
        "    def load_checkpoint(self, filepath, model, optimizer=None, scheduler=None):\n",
        "        \"\"\"Load model checkpoint\"\"\"\n",
        "        if not os.path.exists(filepath):\n",
        "            raise FileNotFoundError(f\"Checkpoint file not found: {filepath}\")\n",
        "        \n",
        "        checkpoint = torch.load(filepath, map_location='cpu')\n",
        "        \n",
        "        # Load model state\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        \n",
        "        # Load optimizer state\n",
        "        if optimizer and 'optimizer_state_dict' in checkpoint:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        \n",
        "        # Load scheduler state\n",
        "        if scheduler and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:\n",
        "            scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        \n",
        "        print(f\"Checkpoint loaded: {filepath}\")\n",
        "        print(f\"Epoch: {checkpoint['epoch']}, Metrics: {checkpoint['metrics']}\")\n",
        "        \n",
        "        return checkpoint\n",
        "    \n",
        "    def get_best_checkpoint_path(self):\n",
        "        \"\"\"Get path to best checkpoint\"\"\"\n",
        "        best_path = os.path.join(self.save_dir, 'best_model.pth')\n",
        "        return best_path if os.path.exists(best_path) else None\n",
        "    \n",
        "    def get_last_checkpoint_path(self):\n",
        "        \"\"\"Get path to last checkpoint\"\"\"\n",
        "        last_path = os.path.join(self.save_dir, 'last_model.pth')\n",
        "        return last_path if os.path.exists(last_path) else None\n",
        "    \n",
        "    def list_checkpoints(self):\n",
        "        \"\"\"List all checkpoints in save directory\"\"\"\n",
        "        pattern = os.path.join(self.save_dir, '*.pth')\n",
        "        checkpoints = glob.glob(pattern)\n",
        "        return sorted(checkpoints)\n",
        "\n",
        "# Example usage\n",
        "def demo_checkpointing():\n",
        "    \"\"\"Demonstrate checkpointing functionality\"\"\"\n",
        "    \n",
        "    # Create a simple model\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(10, 50),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(50, 1)\n",
        "    )\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "    \n",
        "    # Initialize checkpoint manager\n",
        "    checkpoint_manager = ModelCheckpoint(\n",
        "        save_dir='demo_checkpoints',\n",
        "        monitor='val_loss',\n",
        "        mode='min',\n",
        "        save_top_k=3\n",
        "    )\n",
        "    \n",
        "    # Simulate training with checkpointing\n",
        "    for epoch in range(10):\n",
        "        # Simulate training\n",
        "        train_loss = 1.0 * np.exp(-epoch/5) + 0.1 * np.random.random()\n",
        "        val_loss = train_loss + 0.1 + 0.1 * np.random.random()\n",
        "        \n",
        "        metrics = {\n",
        "            'train_loss': train_loss,\n",
        "            'val_loss': val_loss,\n",
        "            'epoch': epoch\n",
        "        }\n",
        "        \n",
        "        # Save checkpoint\n",
        "        additional_state = {'random_seed': 42, 'epoch_data': f'data_for_epoch_{epoch}'}\n",
        "        checkpoint_manager.save_checkpoint(\n",
        "            model, optimizer, scheduler, epoch, metrics, additional_state\n",
        "        )\n",
        "        \n",
        "        scheduler.step()\n",
        "    \n",
        "    # List all checkpoints\n",
        "    print(\"\\nAll checkpoints:\")\n",
        "    for cp in checkpoint_manager.list_checkpoints():\n",
        "        print(f\"  {cp}\")\n",
        "    \n",
        "    # Test loading best checkpoint\n",
        "    best_path = checkpoint_manager.get_best_checkpoint_path()\n",
        "    if best_path:\n",
        "        print(f\"\\nLoading best checkpoint: {best_path}\")\n",
        "        \n",
        "        # Create new model instance\n",
        "        new_model = nn.Sequential(\n",
        "            nn.Linear(10, 50),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(50, 1)\n",
        "        )\n",
        "        new_optimizer = optim.Adam(new_model.parameters(), lr=0.01)\n",
        "        new_scheduler = optim.lr_scheduler.StepLR(new_optimizer, step_size=5, gamma=0.5)\n",
        "        \n",
        "        loaded_checkpoint = checkpoint_manager.load_checkpoint(\n",
        "            best_path, new_model, new_optimizer, new_scheduler\n",
        "        )\n",
        "        \n",
        "        print(f\"Additional state: {loaded_checkpoint.get('additional_state')}\")\n",
        "\n",
        "# Run the demo\n",
        "demo_checkpointing()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### GIF Creation for Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook has demonstrated essential tools and techniques for machine learning development:\n",
        "\n",
        "1. **tqdm** - Progress tracking for training loops\n",
        "2. **Device Management** - Proper GPU/CPU handling\n",
        "3. **argparse** - Command line argument parsing\n",
        "4. **Loss Tracking** - Comprehensive metrics monitoring\n",
        "5. **Evaluation** - Structured model evaluation\n",
        "6. **Computational Efficiency** - Performance profiling\n",
        "7. **TensorBoard** - Training visualization and logging\n",
        "8. **Additional Features** - Network initialization, schedulers, checkpointing, and GIF creation\n",
        "\n",
        "Each section provides practical, reusable code that can be integrated into your machine learning projects. The examples are designed to be simple yet comprehensive, allowing you to understand the core concepts and extend them for your specific needs.\n",
        "\n",
        "**Next Steps:**\n",
        "- Integrate these tools into your existing ML pipeline\n",
        "- Customize the configurations based on your project requirements\n",
        "- Experiment with different initialization strategies and schedulers\n",
        "- Use TensorBoard to monitor and debug your training process\n",
        "- Create visualizations to better understand your model's behavior\n",
        "\n",
        "**Author:** DuhyeonKim + Perplexity (Claude 4)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "d2f",
      "language": "python",
      "name": "d2f"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
