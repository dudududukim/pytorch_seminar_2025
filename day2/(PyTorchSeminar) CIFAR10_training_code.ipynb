{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9837fff",
   "metadata": {},
   "source": [
    "# CIFAR-10 Dataset Training Practice\n",
    "📅 2025.05.21\n",
    "\n",
    "---\n",
    "\n",
    "**copyright © Duhyeon Kim**  \n",
    "*Co-written by ChatGPT & Perplexity & Grok3*  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb814a77",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# PART 1: CIFAR-10 Dataset\n",
    "\n",
    "## 1.1 CIFAR-10 dataset download\n",
    "\n",
    "to download CIFAR10 datset using torchvision, first install the torchvision packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f80904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if your kernel does not have torch and torchvision installed, uncomment the following line\n",
    "# !pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fd4de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True)\n",
    "print(type(trainset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1378fc",
   "metadata": {},
   "source": [
    "## 1.2 CIFAR-10 dataset analysis\n",
    "\n",
    "analyzing the total image num, classes etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b831ab77",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"학습 데이터 크기:\", len(trainset))       # __len__\n",
    "print(trainset[0])\n",
    "print(\"이미지 shape:\", trainset[0][0].size)\n",
    "print(\"이미지 channel:\", trainset[0][0].mode)\n",
    "print(\"이미지 type:\", type(trainset[0][0]))         # to process image in pytorch, PIL.Image.Image should be changed to torch.Tensor\n",
    "print(\"라벨 예시:\", trainset[0][1])\n",
    "print(\"라벨 종류:\", trainset.classes)               # classes variable is defined inside torchvision.datasets.CIFAR10 class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d592164a",
   "metadata": {},
   "source": [
    "- `torchvision.datasets.CIFAR10` inherits from the `torch.utils.data.Dataset` class.\n",
    "\n",
    "- So we can use `__len__()` / `__getitem__()` that we have seen in the lecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445a2694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "len(trainset)\n",
    "plt.imshow(trainset[8][0])\n",
    "plt.title(trainset.classes[trainset[8][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a260269",
   "metadata": {},
   "source": [
    "## 1.3 Saving CIFAR-10 images locally\n",
    "\n",
    "to practice on real data folder format, supposing you have your own data in ./data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8a0871",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# CIFAR-10 dataset load : we have already downloaded the dataset, so we set download=False\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False)\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False)\n",
    "\n",
    "# directory to save images\n",
    "save_root = './cifar10_images/train'\n",
    "os.makedirs(save_root, exist_ok=True)\n",
    "\n",
    "# for training dataset\n",
    "for idx, (img, label) in enumerate(tqdm(train_dataset)):\n",
    "    class_name = train_dataset.classes[label]\n",
    "    class_dir = os.path.join(save_root, class_name)\n",
    "    os.makedirs(class_dir, exist_ok=True)\n",
    "    file_path = os.path.join(class_dir, f'{idx}.png')\n",
    "    if os.path.exists(file_path):\n",
    "        continue\n",
    "    img.save(file_path)\n",
    "\n",
    "# for test dataset\n",
    "for idx, (img, label) in enumerate(tqdm(test_dataset)):\n",
    "    class_name = test_dataset.classes[label]\n",
    "    class_dir = os.path.join('./cifar10_images/test', class_name)\n",
    "    os.makedirs(class_dir, exist_ok=True)\n",
    "    file_path = os.path.join(class_dir, f'{idx}.png')\n",
    "    if os.path.exists(file_path):\n",
    "        continue\n",
    "    img.save(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f2d059",
   "metadata": {},
   "source": [
    "# PART 2: Simplest Training Code\n",
    "\n",
    "📝 using the code that appeared in the lecture silde.\n",
    "\n",
    "- dataset & dataloader\n",
    "- model def (simple cnn & mlp)\n",
    "- loss function (cross Entropy)\n",
    "- optimizer (SGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f73c5c9",
   "metadata": {},
   "source": [
    "## 2.1 `Seed` set\n",
    "\n",
    "`Reproducibility` is extremely important in machine learning experiments. \n",
    "\n",
    "When running the same code, `obtaining identical results` provides the following benefits:\n",
    "\n",
    "- Fair comparison between baseline and improved models\n",
    "\n",
    "- Scientific credibility for research and publications\n",
    "\n",
    "- Debugging ease when issues arise during development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101879e1",
   "metadata": {},
   "source": [
    "> but in jupyter notebook, you should also set the random seed for the notebook (i'm just showing that you can set the seed for reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332e08ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "seed = 42\n",
    "# Python, NumPy, PyTorch -> set random seed\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a17146",
   "metadata": {},
   "source": [
    "## 2.2 Simple Custom `Dataset`\n",
    "\n",
    "(Korean)\n",
    "- 상식적으로 데이터셋 클래스를 지정한다 -> 데이터의 위치를 알아야하고, `idx`로 접근은 해야되기 때문에 해당 이미지들의 경로를 담을 변수가 필요 (이미지를 담아놓으면 메모리 낭비임)\n",
    "- 이때, 경로에 접근해야되기 때문에 폴더의 구조와 특성을 잘 파악해서, 폴더와 이미지만을 가지고 label와 image를 return할 수 있는 `__getitem__` 을 설정해야됨\n",
    "\n",
    "(English)\n",
    "- Typically, you define a custom dataset class, which needs to know the location of the data and must support access by index (`idx`). Instead of loading all images into memory (which is wasteful), you store only the file paths.\n",
    "- To do this, you analyze the folder structure and image files, then implement `__getitem__` so that it returns the image and its label using only the folder and file information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8520eace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "\n",
    "# /Users/kimduhyeon/Desktop/HandS_CV_seminar/day2/cifar10_images\n",
    "\n",
    "class CustomDataset(Dataset):           # any name you want to call (here. CustomDataset)\n",
    "    def __init__(self, root):\n",
    "        self.files = os.listdir(root)\n",
    "        self.root = root\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join(self.root, self.files[idx])\n",
    "        x = open(path, 'rb').read()  # replace with actual processing\n",
    "        return x\n",
    "\n",
    "# where we have saved the images\n",
    "train_root = './cifar10_images/train'\n",
    "test_root = './cifar10_images/test'\n",
    "\n",
    "train_dataset = CustomDataset(root=train_root)          # typically, we use CustomDataset(train_root) w/o the root argument\n",
    "test_dataset = CustomDataset(root=test_root)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fceee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12778c46",
   "metadata": {},
   "source": [
    "### Beep!!! wrong\n",
    "\n",
    "> cifar10_images folder contains the label folder and the images inside that folder.\n",
    "\n",
    "self.files = os.listdir(root) just took the class_name folder, not the files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f870251",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c219bb4b",
   "metadata": {},
   "source": [
    "### So, we need to recursively excess the folder to get the image path.\n",
    "\n",
    "> CIFAR-10 class_to_idx info.\n",
    "\n",
    "{'airplane': 0, 'automobile': 1, 'bird': 2, 'cat': 3, 'deer': 4, 'dog': 5, 'frog': 6, 'horse': 7, 'ship': 8, 'truck': 9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c819b757",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.samples = []\n",
    "        self.class_to_idx = {}\n",
    "        self.idx_to_class = {}\n",
    "\n",
    "        # first, collect all class names and their indices\n",
    "        classes = sorted(os.listdir(root))\n",
    "\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "        self.idx_to_class = {idx: cls_name for cls_name, idx in self.class_to_idx.items()}\n",
    "\n",
    "        # then, collect all image paths and their corresponding labels with repeating the class folders\n",
    "        for cls_name in classes:\n",
    "            cls_folder = os.path.join(root, cls_name)\n",
    "            for fname in os.listdir(cls_folder):\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    path = os.path.join(cls_folder, fname)\n",
    "                    self.samples.append((path, self.class_to_idx[cls_name]))        # adding image path and label here!\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path)          # now we open the image file using PIL\n",
    "        if self.transform:\n",
    "            img = self.transform(img)           # transforming PIL image to torch.Tensor\n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e69999f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_root = './cifar10_images/train'\n",
    "test_root = './cifar10_images/test'\n",
    "\n",
    "train_dataset = CustomDataset(train_root, transform=transform)\n",
    "test_dataset = CustomDataset(test_root, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cb96ea",
   "metadata": {},
   "source": [
    "### Why using .permute(1, 2, 0)\n",
    "\n",
    ">plt.imshow only supports image with (w,h,c) -> channel to the last\n",
    "\n",
    "- permute : swapping the axes of the tensor\n",
    "- cpu : move the tensor to CPU\n",
    "- numpy : convert the tensor to numpy array (formally, it is a tensor, not a numpy array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4300976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[0.0706, 0.1255, 0.1686,  ..., 0.2471, 0.1765, 0.1451],\n",
      "         [0.0784, 0.1176, 0.1765,  ..., 0.2275, 0.1647, 0.0784],\n",
      "         [0.0510, 0.0745, 0.1529,  ..., 0.2157, 0.1451, 0.0588],\n",
      "         ...,\n",
      "         [0.1059, 0.1843, 0.2039,  ..., 0.1373, 0.1451, 0.1137],\n",
      "         [0.0980, 0.1255, 0.1373,  ..., 0.0863, 0.1255, 0.0941],\n",
      "         [0.0078, 0.0118, 0.0078,  ..., 0.1098, 0.1098, 0.0824]],\n",
      "\n",
      "        [[0.1176, 0.1686, 0.2118,  ..., 0.3176, 0.2706, 0.2392],\n",
      "         [0.1255, 0.1569, 0.2196,  ..., 0.3020, 0.2667, 0.1843],\n",
      "         [0.0980, 0.1137, 0.1961,  ..., 0.2902, 0.2471, 0.1686],\n",
      "         ...,\n",
      "         [0.1176, 0.2118, 0.2314,  ..., 0.1922, 0.2078, 0.1765],\n",
      "         [0.1137, 0.1451, 0.1608,  ..., 0.1373, 0.1843, 0.1490],\n",
      "         [0.0196, 0.0196, 0.0157,  ..., 0.1608, 0.1608, 0.1294]],\n",
      "\n",
      "        [[0.1922, 0.2157, 0.2196,  ..., 0.3294, 0.2863, 0.2627],\n",
      "         [0.2000, 0.2118, 0.2392,  ..., 0.3098, 0.3020, 0.2314],\n",
      "         [0.1765, 0.1765, 0.2275,  ..., 0.2980, 0.2824, 0.2196],\n",
      "         ...,\n",
      "         [0.1176, 0.1569, 0.1647,  ..., 0.1490, 0.1569, 0.1294],\n",
      "         [0.1137, 0.1020, 0.1059,  ..., 0.1098, 0.1451, 0.1137],\n",
      "         [0.0392, 0.0471, 0.0588,  ..., 0.1333, 0.1333, 0.1059]]]), 0)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a146d953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'airplane'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_dataset[3][0].permute(1, 2, 0))         # train_dataset[3] returns (image, label)\n",
    "train_dataset.idx_to_class[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad7e5d1",
   "metadata": {},
   "source": [
    "## 2.3 Simple Model (CNN + MLP)\n",
    "\n",
    "> with simple CNNs and MLPs with non-linear activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "783cd8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(16 * 16 * 16, 10)  # 16(out_channel) * 16(width) * 16(height)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))    # relut activation function : x = max(0, x)\n",
    "        x = x.view(-1, 16 * 16 * 16)\n",
    "        # x = nn.Flatten()(x)\n",
    "        x = self.fc1(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d268745",
   "metadata": {},
   "source": [
    "### forward pass example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af82701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomModel(\n",
      "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=4096, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "classifer = CustomModel()\n",
    "print(classifer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4a4c54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0873,  0.0800, -0.0156, -0.0531,  0.0327,  0.0537,  0.1139,  0.0055,\n",
      "          0.1018,  0.0316]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tmp_output = classifer(train_dataset[3][0].unsqueeze(0))  # (c, h, w) -> (1, c, h, w)\n",
    "print(tmp_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "633c69db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "====== Model Output Info ======\n",
      "Shape           : torch.Size([1, 10])\n",
      "Softmax Prob.   :\n",
      "tensor([[0.0891, 0.1053, 0.0957, 0.0922, 0.1004, 0.1026, 0.1089, 0.0978, 0.1076,\n",
      "         0.1003]], grad_fn=<SoftmaxBackward0>)\n",
      "\n",
      "Predicted idex : 6\n",
      "\n",
      "Predicted Class : frog\n",
      "Real label      : airplane\n",
      "==============================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def pretty_print_output(tensor, label = None):\n",
    "    probs = F.softmax(tensor, dim=1)\n",
    "    predicted_idx = probs.argmax(dim=1).item()\n",
    "    print(\"\\n====== Model Output Info ======\")\n",
    "    print(f\"Shape           : {tensor.shape}\")\n",
    "    print(f\"Softmax Prob.   :\\n{probs}\")\n",
    "    print(f\"\\nPredicted idex : {predicted_idx}\")\n",
    "    print(f\"\\nPredicted Class : {train_dataset.idx_to_class[predicted_idx]}\")\n",
    "    print(f\"Real label      : {label}\")\n",
    "    print(\"=\" * 30 + \"\\n\")\n",
    "\n",
    "pretty_print_output(tmp_output, train_dataset.idx_to_class[train_dataset[3][1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832401ff",
   "metadata": {},
   "source": [
    "## 2.4 Loss function\n",
    "\n",
    "- while we uses labeled data and classifier, cross entropy loss is used. <br>\n",
    "`nn.CrossEntropyLoss`\n",
    "- and the optimizer is SGD (Stochastic Gradient Descent) without momentum. (simplest form)\n",
    "`torch.optim.SGD`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a650f3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b150cfb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.4181013107299805\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(tmp_output, torch.tensor([train_dataset[3][1]]))           # train_dataset[3][1] = 6\n",
    "print(f\"Loss: {loss.item()}\")           # .item() returns the value of a tensor as a standard Python number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4d6ac1",
   "metadata": {},
   "source": [
    "## 2.5 Optimizer (steps)\n",
    "0. loss calculation\n",
    "1. `optimizer.zero_grad()`               # zero the gradient buffers\n",
    "2. `loss.backward()`                  # backpropagation\n",
    "3. `optimizer.step()`                 # update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d743ac00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[After optimizer.zero_grad()] Parameter name: conv1.weight\n",
      "  shape: torch.Size([16, 3, 3, 3])\n",
      "  data (first 5 elements): tensor([ 0.1471,  0.1597, -0.0451,  0.1768, -0.0422])\n",
      "  grad (first 5 elements): None\n",
      "\n",
      "[After loss.backward()] Parameter name: conv1.weight\n",
      "  shape: torch.Size([16, 3, 3, 3])\n",
      "  data (first 5 elements): tensor([ 0.1471,  0.1597, -0.0451,  0.1768, -0.0422])\n",
      "  grad (first 5 elements): tensor([0.0546, 0.0687, 0.0394, 0.0591, 0.0750])\n",
      "\n",
      "[After optimizer.step()] Parameter name: conv1.weight\n",
      "  shape: torch.Size([16, 3, 3, 3])\n",
      "  data (first 5 elements): tensor([ 0.1466,  0.1590, -0.0455,  0.1762, -0.0429])\n",
      "  grad (first 5 elements): tensor([0.0546, 0.0687, 0.0394, 0.0591, 0.0750])\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(classifer.parameters(), lr=0.01, momentum=0)\n",
    "name, param = next(iter(classifer.named_parameters()))          # named_parameters() returns an iterator of (name, parameter) tuples\n",
    "\n",
    "def print_param_info(stage, param, name):\n",
    "    print(f\"\\n[{stage}] Parameter name: {name}\")\n",
    "    print(f\"  shape: {param.shape}\")\n",
    "    print(f\"  data (first 5 elements): {param.data.view(-1)[:5]}\")\n",
    "    print(f\"  grad (first 5 elements): {param.grad.view(-1)[:5] if param.grad is not None else None}\")\n",
    "\n",
    "# 1. optimizer.zero_grad()로 grad 초기화\n",
    "optimizer.zero_grad()\n",
    "print_param_info(\"After optimizer.zero_grad()\", param, name)\n",
    "\n",
    "# 2. loss.backward()로 gradient 계산\n",
    "loss.backward()\n",
    "print_param_info(\"After loss.backward()\", param, name)\n",
    "\n",
    "# 3. optimizer.step()으로 파라미터 업데이트\n",
    "optimizer.step()\n",
    "print_param_info(\"After optimizer.step()\", param, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c09509c",
   "metadata": {},
   "source": [
    "# Part3. full training code\n",
    "\n",
    "1. Dataset\n",
    "2. DataLooader\n",
    "3. Model\n",
    "4. Loss function\n",
    "5. Optimizer\n",
    "6. Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6a3ce7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n",
      "Epoch [1/50], Loss: 2.1481\n",
      "Epoch [2/50], Loss: 2.0700\n",
      "Epoch [3/50], Loss: 1.9120\n",
      "Epoch [4/50], Loss: 1.9082\n",
      "Epoch [5/50], Loss: 1.9154\n",
      "Epoch [6/50], Loss: 1.8623\n",
      "Epoch [7/50], Loss: 1.7970\n",
      "Epoch [8/50], Loss: 1.8253\n",
      "Epoch [9/50], Loss: 1.7278\n",
      "Epoch [10/50], Loss: 1.7938\n",
      "Epoch [11/50], Loss: 1.7120\n",
      "Epoch [12/50], Loss: 1.6844\n",
      "Epoch [13/50], Loss: 1.7434\n",
      "Epoch [14/50], Loss: 1.7048\n",
      "Epoch [15/50], Loss: 1.6256\n",
      "Epoch [16/50], Loss: 1.6541\n",
      "Epoch [17/50], Loss: 1.5802\n",
      "Epoch [18/50], Loss: 1.5613\n",
      "Epoch [19/50], Loss: 1.5321\n",
      "Epoch [20/50], Loss: 1.5614\n",
      "Epoch [21/50], Loss: 1.5780\n",
      "Epoch [22/50], Loss: 1.4910\n",
      "Epoch [23/50], Loss: 1.4798\n",
      "Epoch [24/50], Loss: 1.4882\n",
      "Epoch [25/50], Loss: 1.4881\n",
      "Epoch [26/50], Loss: 1.4250\n",
      "Epoch [27/50], Loss: 1.4392\n",
      "Epoch [28/50], Loss: 1.4607\n",
      "Epoch [29/50], Loss: 1.3977\n",
      "Epoch [30/50], Loss: 1.3918\n",
      "Epoch [31/50], Loss: 1.3859\n",
      "Epoch [32/50], Loss: 1.3979\n",
      "Epoch [33/50], Loss: 1.3549\n",
      "Epoch [34/50], Loss: 1.3550\n",
      "Epoch [35/50], Loss: 1.3425\n",
      "Epoch [36/50], Loss: 1.3560\n",
      "Epoch [37/50], Loss: 1.3403\n",
      "Epoch [38/50], Loss: 1.3158\n",
      "Epoch [39/50], Loss: 1.3193\n",
      "Epoch [40/50], Loss: 1.2934\n",
      "Epoch [41/50], Loss: 1.2644\n",
      "Epoch [42/50], Loss: 1.3036\n",
      "Epoch [43/50], Loss: 1.2310\n",
      "Epoch [44/50], Loss: 1.2320\n",
      "Epoch [45/50], Loss: 1.3289\n",
      "Epoch [46/50], Loss: 1.2447\n",
      "Epoch [47/50], Loss: 1.3352\n",
      "Epoch [48/50], Loss: 1.2728\n",
      "Epoch [49/50], Loss: 1.3067\n",
      "Epoch [50/50], Loss: 1.2532\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# 🎯 RANDOM SEED CONFIGURATION\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"        # device=\"mps\" for MacOS, \"cuda\" for GPU, \"cpu\" for CPU\n",
    "print(\"using device:\", device)\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# 🎯 DATASET AND DATALOADER\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.samples = []\n",
    "        self.class_to_idx = {}\n",
    "        self.idx_to_class = {}\n",
    "\n",
    "        # first, collect all class names and their indices\n",
    "        classes = sorted(os.listdir(root))\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "        self.idx_to_class = {idx: cls_name for cls_name, idx in self.class_to_idx.items()} \n",
    "\n",
    "        # then, collect all image paths and their corresponding labels with repeating the class folders\n",
    "        for cls_name in classes:\n",
    "            cls_folder = os.path.join(root, cls_name)\n",
    "            for fname in os.listdir(cls_folder):\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    path = os.path.join(cls_folder, fname)\n",
    "                    self.samples.append((path, self.class_to_idx[cls_name]))        # adding image path and label here!\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path)          # now we open the image file using PIL\n",
    "        if self.transform:\n",
    "            img = self.transform(img)           # transforming PIL image to torch.Tensor\n",
    "        return img, label\n",
    "\n",
    "train_root = './cifar10_images/train'\n",
    "test_root = './cifar10_images/test'\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(train_root, transform=train_transform)\n",
    "test_dataset = CustomDataset(test_root, transform=test_transform)\n",
    "\n",
    "#dataloader\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# 🏗️ MODEL ARCHITECTURE\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(16 * 16 * 16, 10)  # 16(out_channel) * 16(width) * 16(height)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))    # relut activation function : x = max(0, x)\n",
    "        x = x.view(-1, 16 * 16 * 16)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# 📊 MODEL TRAINING\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "classifier = CustomModel().to(device)\n",
    "\n",
    "# training loop \n",
    "total_epochs = 50\n",
    "\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(classifier.parameters(), lr=0.03, momentum=0)\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):             # enumerate returns both index(i) and value( (images, labels) )\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = classifier(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{total_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcfed80",
   "metadata": {},
   "source": [
    "## 3.1 Test Accuarcy on Test dataset\n",
    "\n",
    "> using `test` dataset and test_dataloader evaluate\n",
    "> test_dataset conatains 10,000 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5549dd89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 54.32%\n"
     ]
    }
   ],
   "source": [
    "classifier.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = classifier(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7d81c",
   "metadata": {},
   "source": [
    "# Part4. Improving Model performance\n",
    "\n",
    "### 1. 🏗️ Deeper Model Architecture\n",
    "   - Add more convolutional layers\n",
    "   - Implement batch normalization\n",
    "   - Include dropout for regularization\n",
    "\n",
    "### 2. 📊 Data Augmentation\n",
    "   - Random horizontal flip\n",
    "   - Random crop with padding\n",
    "   - Normalization techniques\n",
    "\n",
    "### 3. ⚡ Optimizer Optimization\n",
    "   - Switch from SGD to Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515de714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n",
      "Epoch [1/50]  Train Loss: 1.4933\n",
      "Epoch [2/50]  Train Loss: 1.1571\n",
      "Epoch [3/50]  Train Loss: 1.0317\n",
      "Epoch [4/50]  Train Loss: 0.9551\n",
      "Epoch [5/50]  Train Loss: 0.8964\n",
      "Epoch [6/50]  Train Loss: 0.8598\n",
      "Epoch [7/50]  Train Loss: 0.8247\n",
      "Epoch [8/50]  Train Loss: 0.7926\n",
      "Epoch [9/50]  Train Loss: 0.7715\n",
      "Epoch [10/50]  Train Loss: 0.7417\n",
      "Epoch [11/50]  Train Loss: 0.7281\n",
      "Epoch [12/50]  Train Loss: 0.7019\n",
      "Epoch [13/50]  Train Loss: 0.6876\n",
      "Epoch [14/50]  Train Loss: 0.6724\n",
      "Epoch [15/50]  Train Loss: 0.6604\n",
      "Epoch [16/50]  Train Loss: 0.6458\n",
      "Epoch [17/50]  Train Loss: 0.6251\n",
      "Epoch [18/50]  Train Loss: 0.6218\n",
      "Epoch [19/50]  Train Loss: 0.6080\n",
      "Epoch [20/50]  Train Loss: 0.5992\n",
      "Epoch [21/50]  Train Loss: 0.5902\n",
      "Epoch [22/50]  Train Loss: 0.5781\n",
      "Epoch [23/50]  Train Loss: 0.5743\n",
      "Epoch [24/50]  Train Loss: 0.5630\n",
      "Epoch [25/50]  Train Loss: 0.5567\n",
      "Epoch [26/50]  Train Loss: 0.5523\n",
      "Epoch [27/50]  Train Loss: 0.5456\n",
      "Epoch [28/50]  Train Loss: 0.5415\n",
      "Epoch [29/50]  Train Loss: 0.5356\n",
      "Epoch [30/50]  Train Loss: 0.5338\n",
      "Epoch [31/50]  Train Loss: 0.5193\n",
      "Epoch [32/50]  Train Loss: 0.5177\n",
      "Epoch [33/50]  Train Loss: 0.5138\n",
      "Epoch [34/50]  Train Loss: 0.5063\n",
      "Epoch [35/50]  Train Loss: 0.5072\n",
      "Epoch [36/50]  Train Loss: 0.5005\n",
      "Epoch [37/50]  Train Loss: 0.5011\n",
      "Epoch [38/50]  Train Loss: 0.4950\n",
      "Epoch [39/50]  Train Loss: 0.4879\n",
      "Epoch [40/50]  Train Loss: 0.4899\n",
      "Epoch [41/50]  Train Loss: 0.4853\n",
      "Epoch [42/50]  Train Loss: 0.4839\n",
      "Epoch [43/50]  Train Loss: 0.4802\n",
      "Epoch [44/50]  Train Loss: 0.4736\n",
      "Epoch [45/50]  Train Loss: 0.4717\n",
      "Epoch [46/50]  Train Loss: 0.4700\n",
      "Epoch [47/50]  Train Loss: 0.4683\n",
      "Epoch [48/50]  Train Loss: 0.4640\n",
      "Epoch [49/50]  Train Loss: 0.4655\n",
      "Epoch [50/50]  Train Loss: 0.4669\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 162\u001b[0m\n\u001b[1;32m    158\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(avg_loss)\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]  Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 162\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mplot(train_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    163\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    164\u001b[0m plt\u001b[38;5;241m.\u001b[39mylabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# 🎯 RANDOM SEED CONFIGURATION\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"        # device=\"mps\" for MacOS, \"cuda\" for GPU, \"cpu\" for CPU\n",
    "print(\"using device:\", device)\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# 🎯 DATASET AND DATALOADER\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.samples = []\n",
    "        self.class_to_idx = {}\n",
    "        self.idx_to_class = {}\n",
    "\n",
    "        # first, collect all class names and their indices\n",
    "        classes = sorted(os.listdir(root))\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "        self.idx_to_class = {idx: cls_name for cls_name, idx in self.class_to_idx.items()} \n",
    "\n",
    "        # then, collect all image paths and their corresponding labels with repeating the class folders\n",
    "        for cls_name in classes:\n",
    "            cls_folder = os.path.join(root, cls_name)\n",
    "            for fname in os.listdir(cls_folder):\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    path = os.path.join(cls_folder, fname)\n",
    "                    self.samples.append((path, self.class_to_idx[cls_name]))        # adding image path and label here!\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path)          # now we open the image file using PIL\n",
    "        if self.transform:\n",
    "            img = self.transform(img)           # transforming PIL image to torch.Tensor\n",
    "        return img, label\n",
    "\n",
    "train_root = './cifar10_images/train'\n",
    "test_root = './cifar10_images/test'\n",
    "\n",
    "# MODIFIED: Enhanced data augmentation techniques and added normalization\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(32, padding=4),  # MODIFIED: Added random crop\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # MODIFIED: cifar10 standard normalization\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # MODIFIED: Added normalization (same as train, cause model is trained with this normalization)\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(train_root, transform=train_transform)\n",
    "test_dataset = CustomDataset(test_root, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# 🏗️ MODEL ARCHITECTURE\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class CustomModel_v2(nn.Module):\n",
    "    def __init__(self, num_classes=10):  # MODIFIED: Made number of classes as parameter\n",
    "        super(CustomModel_v2, self).__init__()\n",
    "        \n",
    "        # MODIFIED: Improved with deeper network architecture\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)  # MODIFIED: Added Batch Normalization\n",
    "\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)  # MODIFIED: Added Batch Normalization\n",
    "        \n",
    "        # MODIFIED: Additional convolutional layer\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # MODIFIED: Adjusted dropout rate and applied to multiple locations\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        # MODIFIED: Increased fully connected layer size\n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)  # 32x32 -> 16x16 -> 8x8 -> 4x4\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # MODIFIED: Improved forward pass with Batch Normalization\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))  # 32x32 -> 16x16\n",
    "\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))  # 16x16 -> 8x8\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))  # 8x8 -> 4x4\n",
    "        \n",
    "        x = self.dropout1(x)\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# MODIFIED: Calculate number of classes\n",
    "num_classes = len(train_dataset.class_to_idx)\n",
    "model = CustomModel_v2(num_classes=num_classes).to(device)          # naming `model` is proper than `classifier` for clarity\n",
    "\n",
    "total_epochs = 50\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# ‼️ optimizer modi SGD to Adam for better convergence\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "train_losses = []  # MODIFIED: List to store training losses\n",
    "\n",
    "for epoch in range(total_epochs):\n",
    "    model.train()\n",
    "    # MODIFIED: added running loss for each epoch\n",
    "    running_loss = 0.0\n",
    "    for i, (images, labels) in enumerate(train_loader):             # enumerate returns both index(i) and value( (images, labels) )\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{total_epochs}]  Train Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60809bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b4282fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 83.47%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c130d17",
   "metadata": {},
   "source": [
    "## 4.2 checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf31924",
   "metadata": {},
   "source": [
    "> saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "617e659a",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model_checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5884f098",
   "metadata": {},
   "source": [
    "> loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "316efa69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loaded = CustomModel_v2(num_classes=num_classes).to(device)  # create a new instance of the model\n",
    "checkpoint = torch.load('model_checkpoint.pth')\n",
    "# print(checkpoint)\n",
    "model_loaded.load_state_dict(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0289768",
   "metadata": {},
   "source": [
    "> onnx form saving\n",
    "\n",
    "you can visualize in the [Netron](https://netron.app/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e33d498b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.randn(1, 3, 32, 32).to(device)  # 입력 크기와 맞춰서 설정\n",
    "torch.onnx.export(model, dummy_input, 'model_checkpoint.onnx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9554ed38",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Part5. (Day3) Practical Tools in Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699c089c",
   "metadata": {},
   "source": [
    "## 5.1 tqdm + compute_effi + scheduler + optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4e14c26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1562/1563 [00:41<00:00, 36.70it/s, Loss=0.8897, compute effi.=95.13%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50]  Train Loss: 1.5042  Avg Compute Efficiency: 47.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1563/1563 [00:41<00:00, 37.77it/s, Loss=0.8897, compute effi.=95.13%]\n",
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "  6%|▌         | 96/1563 [00:02<00:44, 32.89it/s, Loss=1.2534, compute effi.=82.15%]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training interrupted by user.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# 🎯 RANDOM SEED CONFIGURATION\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(\"using device:\", device)\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# 🎯 DATASET AND DATALOADER\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.samples = []\n",
    "        self.class_to_idx = {}\n",
    "        self.idx_to_class = {}\n",
    "\n",
    "        # first, collect all class names and their indices\n",
    "        classes = sorted(os.listdir(root))\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "        self.idx_to_class = {idx: cls_name for cls_name, idx in self.class_to_idx.items()} \n",
    "\n",
    "        # then, collect all image paths and their corresponding labels with repeating the class folders\n",
    "        for cls_name in classes:\n",
    "            cls_folder = os.path.join(root, cls_name)\n",
    "            for fname in os.listdir(cls_folder):\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    path = os.path.join(cls_folder, fname)\n",
    "                    self.samples.append((path, self.class_to_idx[cls_name]))        # adding image path and label here!\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path)          # now we open the image file using PIL\n",
    "        if self.transform:\n",
    "            img = self.transform(img)           # transforming PIL image to torch.Tensor\n",
    "        return img, label\n",
    "\n",
    "train_root = './cifar10_images/train'\n",
    "test_root = './cifar10_images/test'\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(train_root, transform=train_transform)\n",
    "test_dataset = CustomDataset(test_root, transform=test_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# 🏗️ MODEL ARCHITECTURE\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class CustomModel_v2(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomModel_v2, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "    \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        x = self.dropout1(x)\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "num_classes = len(train_dataset.class_to_idx)\n",
    "model = CustomModel_v2(num_classes=num_classes).to(device)\n",
    "\n",
    "total_epochs = 50\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "train_losses = []\n",
    "\n",
    "try:\n",
    "    for epoch in range(total_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        compute_effi = 0.0\n",
    "\n",
    "        load_start = time.time()\n",
    "\n",
    "        pbar = tqdm(total=len(train_loader))\n",
    "\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            load_time = time.time() - load_start\n",
    "            compute_start = time.time()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            compute_time = time.time() - compute_start\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            computational_efficiency = compute_time / (load_time + compute_time)\n",
    "            compute_effi += computational_efficiency\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                'Loss' : f\"{loss.item():.4f}\",\n",
    "                'compute effi.' : f\"{(computational_efficiency)*100:.2f}%\",\n",
    "            })\n",
    "            pbar.update(1)\n",
    "            load_start = time.time()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        avg_compute_effi = compute_effi / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{total_epochs}]  Train Loss: {avg_loss:.4f}  Avg Compute Efficiency: {avg_compute_effi*100:.2f}%\")\n",
    "    pbar.close()\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    pbar.close()\n",
    "    print(\"\\nTraining interrupted by user.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd792619",
   "metadata": {},
   "source": [
    "## 5.2 Completed code (applied things that we have learned so far)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a993d915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[1/50]: 100%|██████████| 1407/1407 [00:23<00:00, 59.76it/s, Loss=0.6370, Compute_efficiency=1.00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.5249 | Avg Computational Effi: 0.915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [1/50]: 100%|██████████| 157/157 [00:02<00:00, 67.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.2616 | Accuracy(val): 54.54%\n",
      "Best model saved at epoch 1 with accuracy 54.54%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[2/50]: 100%|██████████| 1407/1407 [00:22<00:00, 62.12it/s, Loss=0.7729, Compute_efficiency=0.90]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2068 | Avg Computational Effi: 0.921\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [2/50]: 100%|██████████| 157/157 [00:02<00:00, 64.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 1.0654 | Accuracy(val): 62.74%\n",
      "Best model saved at epoch 2 with accuracy 62.74%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[3/50]: 100%|██████████| 1407/1407 [00:24<00:00, 58.26it/s, Loss=0.5956, Compute_efficiency=0.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0711 | Avg Computational Effi: 0.919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [3/50]: 100%|██████████| 157/157 [00:02<00:00, 62.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.9732 | Accuracy(val): 65.08%\n",
      "Best model saved at epoch 3 with accuracy 65.08%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[4/50]: 100%|██████████| 1407/1407 [00:28<00:00, 50.23it/s, Loss=1.0221, Compute_efficiency=0.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9877 | Avg Computational Effi: 0.926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [4/50]: 100%|██████████| 157/157 [00:02<00:00, 59.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.9302 | Accuracy(val): 67.18%\n",
      "Best model saved at epoch 4 with accuracy 67.18%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[5/50]: 100%|██████████| 1407/1407 [00:37<00:00, 37.30it/s, Loss=0.8818, Compute_efficiency=0.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.9310 | Avg Computational Effi: 0.933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [5/50]: 100%|██████████| 157/157 [00:03<00:00, 51.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.8672 | Accuracy(val): 70.16%\n",
      "Best model saved at epoch 5 with accuracy 70.16%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[6/50]: 100%|██████████| 1407/1407 [00:38<00:00, 36.71it/s, Loss=0.6606, Compute_efficiency=0.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8785 | Avg Computational Effi: 0.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [6/50]: 100%|██████████| 157/157 [00:03<00:00, 50.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.8110 | Accuracy(val): 72.86%\n",
      "Best model saved at epoch 6 with accuracy 72.86%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[7/50]: 100%|██████████| 1407/1407 [00:34<00:00, 40.92it/s, Loss=0.6445, Compute_efficiency=0.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8422 | Avg Computational Effi: 0.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [7/50]: 100%|██████████| 157/157 [00:03<00:00, 51.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.7970 | Accuracy(val): 72.66%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[8/50]: 100%|██████████| 1407/1407 [00:31<00:00, 44.13it/s, Loss=0.3354, Compute_efficiency=0.91]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.8070 | Avg Computational Effi: 0.939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [8/50]: 100%|██████████| 157/157 [00:02<00:00, 56.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.7981 | Accuracy(val): 72.34%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[9/50]: 100%|██████████| 1407/1407 [00:31<00:00, 45.06it/s, Loss=0.8739, Compute_efficiency=0.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7768 | Avg Computational Effi: 0.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [9/50]: 100%|██████████| 157/157 [00:02<00:00, 65.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.6948 | Accuracy(val): 76.36%\n",
      "Best model saved at epoch 9 with accuracy 76.36%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[10/50]: 100%|██████████| 1407/1407 [00:26<00:00, 53.47it/s, Loss=0.6147, Compute_efficiency=0.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.7579 | Avg Computational Effi: 0.940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [10/50]: 100%|██████████| 157/157 [00:03<00:00, 41.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.6976 | Accuracy(val): 75.52%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[11/50]: 100%|██████████| 1407/1407 [00:29<00:00, 48.39it/s, Loss=1.2567, Compute_efficiency=0.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6569 | Avg Computational Effi: 0.939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [11/50]: 100%|██████████| 157/157 [00:02<00:00, 55.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.6233 | Accuracy(val): 78.44%\n",
      "Best model saved at epoch 11 with accuracy 78.44%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[12/50]: 100%|██████████| 1407/1407 [00:33<00:00, 41.55it/s, Loss=0.5375, Compute_efficiency=0.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6317 | Avg Computational Effi: 0.939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [12/50]: 100%|██████████| 157/157 [00:03<00:00, 51.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.6051 | Accuracy(val): 79.64%\n",
      "Best model saved at epoch 12 with accuracy 79.64%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[13/50]: 100%|██████████| 1407/1407 [00:32<00:00, 43.45it/s, Loss=0.4562, Compute_efficiency=0.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6196 | Avg Computational Effi: 0.940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [13/50]: 100%|██████████| 157/157 [00:03<00:00, 49.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5897 | Accuracy(val): 79.56%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[14/50]: 100%|██████████| 1407/1407 [00:45<00:00, 31.21it/s, Loss=1.0351, Compute_efficiency=0.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.6064 | Avg Computational Effi: 0.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [14/50]: 100%|██████████| 157/157 [00:03<00:00, 40.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5799 | Accuracy(val): 79.92%\n",
      "Best model saved at epoch 14 with accuracy 79.92%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[15/50]: 100%|██████████| 1407/1407 [00:54<00:00, 25.64it/s, Loss=0.2943, Compute_efficiency=0.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5970 | Avg Computational Effi: 0.934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [15/50]: 100%|██████████| 157/157 [00:03<00:00, 42.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5792 | Accuracy(val): 79.90%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[16/50]: 100%|██████████| 1407/1407 [00:53<00:00, 26.31it/s, Loss=0.9822, Compute_efficiency=0.93]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5846 | Avg Computational Effi: 0.934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [16/50]: 100%|██████████| 157/157 [00:04<00:00, 39.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5715 | Accuracy(val): 80.06%\n",
      "Best model saved at epoch 16 with accuracy 80.06%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[17/50]: 100%|██████████| 1407/1407 [00:53<00:00, 26.14it/s, Loss=0.8188, Compute_efficiency=0.94]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5827 | Avg Computational Effi: 0.933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [17/50]: 100%|██████████| 157/157 [00:03<00:00, 39.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5619 | Accuracy(val): 81.08%\n",
      "Best model saved at epoch 17 with accuracy 81.08%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[18/50]: 100%|██████████| 1407/1407 [01:01<00:00, 23.05it/s, Loss=0.5504, Compute_efficiency=0.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5726 | Avg Computational Effi: 0.936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [18/50]: 100%|██████████| 157/157 [00:04<00:00, 39.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5619 | Accuracy(val): 80.90%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[19/50]: 100%|██████████| 1407/1407 [00:59<00:00, 23.51it/s, Loss=1.2039, Compute_efficiency=0.95]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5689 | Avg Computational Effi: 0.935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [19/50]: 100%|██████████| 157/157 [00:03<00:00, 40.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5470 | Accuracy(val): 80.92%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[20/50]: 100%|██████████| 1407/1407 [01:00<00:00, 23.13it/s, Loss=0.9593, Compute_efficiency=0.96]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5638 | Avg Computational Effi: 0.934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [20/50]: 100%|██████████| 157/157 [00:03<00:00, 42.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5614 | Accuracy(val): 80.60%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[21/50]: 100%|██████████| 1407/1407 [01:05<00:00, 21.55it/s, Loss=0.4882, Compute_efficiency=0.92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5507 | Avg Computational Effi: 0.936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [21/50]: 100%|██████████| 157/157 [00:03<00:00, 41.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5487 | Accuracy(val): 81.24%\n",
      "Best model saved at epoch 21 with accuracy 81.24%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[22/50]: 100%|██████████| 1407/1407 [01:06<00:00, 21.03it/s, Loss=0.4643, Compute_efficiency=0.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5457 | Avg Computational Effi: 0.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [22/50]: 100%|██████████| 157/157 [00:03<00:00, 44.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5463 | Accuracy(val): 81.94%\n",
      "Best model saved at epoch 22 with accuracy 81.94%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[23/50]: 100%|██████████| 1407/1407 [01:06<00:00, 21.19it/s, Loss=0.6494, Compute_efficiency=0.89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5482 | Avg Computational Effi: 0.936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [23/50]: 100%|██████████| 157/157 [00:03<00:00, 41.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5525 | Accuracy(val): 80.94%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[24/50]: 100%|██████████| 1407/1407 [01:03<00:00, 22.03it/s, Loss=1.0424, Compute_efficiency=0.86]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5457 | Avg Computational Effi: 0.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [24/50]: 100%|██████████| 157/157 [00:03<00:00, 41.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5499 | Accuracy(val): 81.66%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[25/50]: 100%|██████████| 1407/1407 [01:06<00:00, 21.11it/s, Loss=0.6259, Compute_efficiency=0.97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.5476 | Avg Computational Effi: 0.939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation [25/50]: 100%|██████████| 157/157 [00:04<00:00, 37.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5464 | Accuracy(val): 81.24%\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[26/50]:  75%|███████▍  | 1053/1407 [00:53<00:17, 19.77it/s, Loss=0.4590, Compute_efficiency=0.99]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training interrupted by user.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "# [NEW] imports for 5.1\n",
    "from tqdm import tqdm                   # for tqdm\n",
    "import time                             # for computation efficiency\n",
    "from torch.utils.data import Subset     # for Subset\n",
    "from prefetch_generator import BackgroundGenerator\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# 🎯 RANDOM SEED CONFIGURATION\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "print(\"using device:\", device)\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# 🎯 DATASET AND DATALOADER\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.samples = []\n",
    "        self.class_to_idx = {}\n",
    "        self.idx_to_class = {}\n",
    "\n",
    "        # first, collect all class names and their indices\n",
    "        classes = sorted(os.listdir(root))\n",
    "        self.class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}\n",
    "        self.idx_to_class = {idx: cls_name for cls_name, idx in self.class_to_idx.items()} \n",
    "\n",
    "        # then, collect all image paths and their corresponding labels with repeating the class folders\n",
    "        for cls_name in classes:\n",
    "            cls_folder = os.path.join(root, cls_name)\n",
    "            for fname in os.listdir(cls_folder):\n",
    "                if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    path = os.path.join(cls_folder, fname)\n",
    "                    self.samples.append((path, self.class_to_idx[cls_name]))        # adding image path and label here!\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        img = Image.open(path)          # now we open the image file using PIL\n",
    "        if self.transform:\n",
    "            img = self.transform(img)           # transforming PIL image to torch.Tensor\n",
    "        return img, label\n",
    "\n",
    "train_root = './cifar10_images/train'\n",
    "test_root = './cifar10_images/test'\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "])\n",
    "\n",
    "train_dataset = CustomDataset(train_root, transform=train_transform)\n",
    "test_dataset = CustomDataset(test_root, transform=test_transform)\n",
    "\n",
    "# [NEW] Validation dataset\n",
    "num_samples = len(train_dataset)\n",
    "indices = np.arange(num_samples)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "split = int(np.floor(0.1 * num_samples))\n",
    "val_indices = indices[:split]\n",
    "train_indices = indices[split:]\n",
    "\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "val_subset = Subset(train_dataset, val_indices)\n",
    "\n",
    "train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "# 🏗️ MODEL ARCHITECTURE\n",
    "# ═════════════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class CustomModel_v2(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomModel_v2, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "    \n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc1 = nn.Linear(128 * 4 * 4, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        \n",
    "        x = self.dropout1(x)\n",
    "        x = x.view(-1, 128 * 4 * 4)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "num_classes = len(train_dataset.class_to_idx)\n",
    "model = CustomModel_v2(num_classes=num_classes).to(device)\n",
    "\n",
    "total_epochs = 50\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# [NEW] Learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "train_losses = []\n",
    "best_val_accuracy = 0.0\n",
    "\n",
    "try:\n",
    "    for epoch in range(total_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        compute_efficiencies = []\n",
    "\n",
    "        # [NEW] tqdm progress bar\n",
    "        pbar = tqdm(total=len(train_loader), desc=f'Epoch[{epoch + 1}/{total_epochs}]')\n",
    "\n",
    "        # [NEW] computation efficiency\n",
    "        start_time = time.time()\n",
    "\n",
    "        # [NEW] BackgroundGenerator for prefetching\n",
    "        for i, (images, labels) in enumerate(BackgroundGenerator(train_loader)):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            prepare_time = time.time() - start_time\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            compute_time = time.time() - start_time - prepare_time\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            compute_efficiency = compute_time / (prepare_time + compute_time)\n",
    "            compute_efficiencies.append(compute_efficiency)\n",
    "\n",
    "            pbar.set_postfix({\n",
    "                'Loss' : f'{loss.item():.4f}',\n",
    "                'Compute_efficiency' : f'{compute_efficiency:.2f}',\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            pbar.update(1)\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        average_compute_efficiency = sum(compute_efficiencies) / len(compute_efficiencies) if compute_efficiencies else 0\n",
    "\n",
    "        print(f\"Train Loss: {avg_loss:.4f} | Avg Computational Effi: {average_compute_efficiency:.3f}\")\n",
    "\n",
    "        # [NEW] Validation loop\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pbar = tqdm(total=len(val_loader), desc=f'Validation [{epoch + 1}/{total_epochs}]')\n",
    "            for images, labels in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                pbar.update(1)\n",
    "                \n",
    "        pbar.close()\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = 100 * correct / total\n",
    "\n",
    "        print(f\"Val loss: {val_loss:.4f} | Accuracy(val): {val_accuracy:.2f}%\")\n",
    "    \n",
    "        # [NEW] Best model checkpoint\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            # pt save\n",
    "            torch.save(model.state_dict(), \"best_model.pt\")\n",
    "            # onnx export\n",
    "            dummy_input = torch.randn(1, 3, 32, 32, device=device)\n",
    "            torch.onnx.export(model, dummy_input, \"best_model.onnx\")\n",
    "            print(f\"Best model saved at epoch {epoch+1} with accuracy {best_val_accuracy:.2f}%\")\n",
    "\n",
    "        print(\"\\n\")\n",
    "\n",
    "        # [NEW] Step the learning rate scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    pbar.close()\n",
    "    print(\"\\nTraining interrupted by user.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6301dfc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2f",
   "language": "python",
   "name": "d2f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
